import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split

# Load data
privacy_descriptions = [
    "An eLearning platform provides users with clear and transparent privacy policies, ensuring that their data is securely managed and protected from unauthorized access.",
    "An eLearning platform collects and stores user data, including personal information such as names, emails, and addresses, to provide personalized learning experiences.",
    # Add more privacy descriptions here...
]

sentiments = ['Positive', 'Negative'] * (len(privacy_descriptions) // 2)

# Tokenize data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
encoded_data = tokenizer(privacy_descriptions, padding=True, truncation=True, return_tensors='pt')

input_ids = encoded_data['input_ids']
attention_masks = encoded_data['attention_mask']
labels = torch.tensor([sentiments.index(sentiment) for sentiment in sentiments])

# Split data into train, validation, and test sets
train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.1, random_state=42)
train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    train_inputs, train_masks, train_labels, test_size=0.1, random_state=42)

# Create DataLoaders
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_dataloader = DataLoader(val_data, batch_size=8, shuffle=False)

test_data = TensorDataset(test_inputs, test_masks, test_labels)
test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False)

# Define sentiment analysis model
sentiment_model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=len(sentiments),
    output_attentions=False,
    output_hidden_states=False
)

# Fine-tune sentiment analysis model
optimizer = AdamW(sentiment_model.parameters(), lr=2e-5, eps=1e-8)
loss_fn = nn.CrossEntropyLoss()

num_epochs = 3
for epoch in range(num_epochs):
    sentiment_model.train()
    total_loss = 0

    for batch in train_dataloader:
        batch_input_ids, batch_attention_mask, batch_labels = batch

        optimizer.zero_grad()

        outputs = sentiment_model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    # Validation
    sentiment_model.eval()
    val_accuracy = 0
    for batch in val_dataloader:
        batch_input_ids, batch_attention_mask, batch_labels = batch
        with torch.no_grad():
            outputs = sentiment_model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1)
        val_accuracy += torch.sum(preds == batch_labels).item()

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}, Val Accuracy: {val_accuracy / len(val_data)}')

# Test the sentiment analysis model
sentiment_model.eval()
test_accuracy = 0
for batch in test_dataloader:
    batch_input_ids, batch_attention_mask, batch_labels = batch
    with torch.no_grad():
        outputs = sentiment_model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1)
    test_accuracy += torch.sum(preds == batch_labels).item()

print(f'Test Accuracy: {test_accuracy / len(test_data)}')

# Now, for texts predicted as negative sentiment, we can train another classifier for privacy issue prediction.

# Filter out negative sentiment texts
negative_indices = [i for i, sentiment in enumerate(sentiments) if sentiment == 'Negative']
negative_texts = [privacy_descriptions[i] for i in negative_indices]

# Train a privacy issue classifier
privacy_model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=3,  # Assuming 3 privacy issues: Data Leakage, Identity theft, Location Tracking
    output_attentions=False,
    output_hidden_states=False
)

# Tokenize negative sentiment texts
encoded_negative_texts = tokenizer(negative_texts, padding=True, truncation=True, return_tensors='pt')
input_ids_negative = encoded_negative_texts['input_ids']
attention_masks_negative = encoded_negative_texts['attention_mask']
labels_negative = torch.tensor([0, 1, 2] * (len(negative_texts) // 3))  # Example labels, replace with actual labels

# Split data into train, validation, and test sets
train_inputs_neg, test_inputs_neg, train_masks_neg, test_masks_neg, train_labels_neg, test_labels_neg = train_test_split(
    input_ids_negative, attention_masks_negative, labels_negative, test_size=0.1, random_state=42)
train_inputs_neg, val_inputs_neg, train_masks_neg, val_masks_neg, train_labels_neg, val_labels_neg = train_test_split(
    train_inputs_neg, train_masks_neg, train_labels_neg, test_size=0.1, random_state=42)

# Create DataLoaders
train_data_neg = TensorDataset(train_inputs_neg, train_masks_neg, train_labels_neg)
train_dataloader_neg = DataLoader(train_data_neg, batch_size=8, shuffle=True)

val_data_neg = TensorDataset(val_inputs_neg, val_masks_neg, val_labels_neg)
val_dataloader_neg = DataLoader(val_data_neg, batch_size=8, shuffle=False)

test_data_neg = TensorDataset(test_inputs_neg, test_masks_neg, test_labels_neg)
test_dataloader_neg = DataLoader(test_data_neg, batch_size=8, shuffle=False)

# Fine-tune privacy issue classifier
optimizer_neg = AdamW(privacy_model.parameters(), lr=2e-5, eps=1e-8)
loss_fn_neg = nn.CrossEntropyLoss()

num_epochs = 3
for epoch in range(num_epochs):
    privacy_model.train()
    total_loss = 0

    for batch in train_dataloader_neg:
        batch_input_ids, batch_attention_mask, batch_labels = batch

        optimizer_neg.zero_grad()

        outputs = privacy_model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer_neg.step()

    # Validation
    privacy_model.eval()
    val_accuracy = 0
    for batch in val_dataloader_neg:
        batch_input_ids, batch_attention_mask, batch_labels = batch
        with torch.no_grad():
            outputs = privacy_model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1)
        val_accuracy += torch.sum(preds == batch_labels).item()

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}, Val Accuracy: {val_accuracy / len(val_data_neg)}')

# Test the privacy issue classifier
privacy_model.eval()
test_accuracy = 0
for batch in test_dataloader_neg:
    batch_input_ids, batch_attention_mask, batch_labels = batch
    with torch.no_grad():
        outputs = privacy_model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1)
    test_accuracy += torch.sum(preds == batch_labels).item()

print(f'Test Accuracy: {test_accuracy / len(test_data_neg)}')
